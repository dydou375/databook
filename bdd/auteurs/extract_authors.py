#!/usr/bin/env python3
"""
Script pour extraire les cl√©s uniques et noms d'auteurs depuis le fichier OpenLibrary
ol_dump_authors_2025-05-31.txt pour insertion en base de donn√©es.
"""

import json
import csv
import gzip
import re
from pathlib import Path
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import pandas as pd
from tqdm import tqdm

def extract_authors_from_dump():
    """
    Extrait les cl√©s uniques et noms d'auteurs depuis le fichier dump OpenLibrary.
    Sauvegarde les r√©sultats dans un fichier CSV.
    """
    
    # Chemins des fichiers
    input_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\ol_dump_authors_2025-05-31.txt.gz")
    output_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\extracted_authors.csv")
    
    print(f"Lecture du fichier : {input_file}")
    print(f"Sauvegarde vers : {output_file}")
    
    # Compteurs pour le suivi
    total_lines = 0
    extracted_authors = 0
    errors = 0
    
    # Liste pour stocker les auteurs extraits
    authors_data = []
    
    try:
        # Ouvrir le fichier compress√©
        with gzip.open(input_file, 'rt', encoding='utf-8') as file:
            for line_num, line in enumerate(file, 1):
                total_lines += 1
                
                # Afficher le progr√®s tous les 10000 lignes
                if line_num % 10000 == 0:
                    print(f"Traitement ligne {line_num:,}...")
                
                try:
                    # S√©parer les colonnes (tab-separated)
                    parts = line.strip().split('\t')
                    
                    if len(parts) >= 5:
                        type_field = parts[0]
                        author_key = parts[1]
                        json_data = parts[4]
                        
                        # V√©rifier que c'est bien un auteur
                        if type_field == "/type/author" and author_key.startswith("/authors/"):
                            try:
                                # Parser le JSON
                                author_data = json.loads(json_data)
                                
                                # Extraire le nom de l'auteur
                                if "name" in author_data:
                                    author_name = author_data["name"]
                                    
                                    # Nettoyer les donn√©es
                                    author_key_clean = author_key.replace("/authors/", "")
                                    author_name_clean = str(author_name).strip()
                                    
                                    # Ajouter √† la liste
                                    authors_data.append({
                                        'author_key': author_key_clean,
                                        'author_name': author_name_clean,
                                        'full_key': author_key
                                    })
                                    
                                    extracted_authors += 1
                                    
                            except json.JSONDecodeError:
                                errors += 1
                                continue
                                
                except Exception as e:
                    errors += 1
                    continue
                    
                # Limiter pour les tests (retirer cette condition pour traitement complet)
                #if line_num >= 100000:  # Traiter seulement les 100k premi√®res lignes pour test
                #    print("Mode test : arr√™t apr√®s 100k lignes")
                #    break
    
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier : {e}")
        return
    
    # Sauvegarder dans un fichier CSV
    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['author_key', 'author_name', 'full_key']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            # √âcrire l'en-t√™te
            writer.writeheader()
            
            # √âcrire les donn√©es
            for author in authors_data:
                writer.writerow(author)
        
        print(f"\n=== R√âSULTATS ===")
        print(f"Lignes trait√©es : {total_lines:,}")
        print(f"Auteurs extraits : {extracted_authors:,}")
        print(f"Erreurs : {errors:,}")
        print(f"Fichier sauvegard√© : {output_file}")
        
        # Afficher quelques exemples
        print(f"\n=== EXEMPLES D'AUTEURS EXTRAITS ===")
        for i, author in enumerate(authors_data[:10]):
            print(f"{i+1}. Cl√©: {author['author_key']} | Nom: {author['author_name']}")
            
    except Exception as e:
        print(f"Erreur lors de la sauvegarde : {e}")

def create_sql_insert_file():
    """
    Cr√©e un fichier SQL avec les instructions INSERT pour la table auteurs.
    """
    input_csv = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\extracted_authors.csv")
    output_sql = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\insert_authors.sql")
    
    if not input_csv.exists():
        print(f"Le fichier {input_csv} n'existe pas. Ex√©cutez d'abord extract_authors_from_dump()")
        return
    
    try:
        with open(input_csv, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            with open(output_sql, 'w', encoding='utf-8') as sqlfile:
                sqlfile.write("-- Instructions SQL pour ins√©rer les auteurs\n")
                sqlfile.write("-- Table suppos√©e : auteur (id_auteur VARCHAR, nom VARCHAR, date_naissance DATE, biographie TEXT)\n\n")
                
                for row in reader:
                    author_key = row['author_key'].replace("'", "''")  # √âchapper les apostrophes
                    author_name = row['author_name'].replace("'", "''")  # √âchapper les apostrophes
                    
                    sql = f"INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) VALUES ('{author_key}', '{author_name}', NULL, NULL);\n"
                    sqlfile.write(sql)
        
        print(f"Fichier SQL cr√©√© : {output_sql}")
        
    except Exception as e:
        print(f"Erreur lors de la cr√©ation du fichier SQL : {e}")

def insert_authors_to_database():
    """
    Ins√®re les auteurs du fichier CSV dans la base de donn√©es PostgreSQL avec SQLAlchemy.
    """
    csv_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\extracted_authors.csv")
    
    if not csv_file.exists():
        print(f"Le fichier CSV {csv_file} n'existe pas.")
        print("Ex√©cutez d'abord extract_authors_from_dump() pour cr√©er le fichier.")
        return
    
    # Param√®tres de connexion √† la base de donn√©es
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    print("=== INSERTION DANS LA BASE DE DONN√âES (SQLAlchemy) ===")
    print(f"Lecture du fichier CSV : {csv_file}")
    
    try:
        # Cr√©er l'engine SQLAlchemy
        engine = create_engine(DATABASE_URL)
        Session = sessionmaker(bind=engine)
        session = Session()
        
        print("Connexion √† la base de donn√©es...")
        
        # Compter d'abord le nombre total de lignes pour la barre de progression
        print("üìä Comptage des auteurs √† traiter...")
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            total_authors = sum(1 for line in csvfile) - 1  # -1 pour l'en-t√™te
        
        print(f"üìà {total_authors:,} auteurs √† traiter")
        
        # Lire le fichier CSV avec barre de progression
        authors_inserted = 0
        authors_skipped = 0
        
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            # Barre de progression avec tqdm
            with tqdm(total=total_authors, desc="üîÑ Insertion auteurs", unit="auteurs", 
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
                
                for row in reader:
                    author_key = row['author_key']
                    author_name = row['author_name']
                    
                    try:
                        # V√©rifier si l'auteur existe d√©j√†
                        result = session.execute(
                            text("SELECT COUNT(*) FROM auteur WHERE id_auteur = :id_auteur"),
                            {"id_auteur": author_key}
                        ).scalar()
                        
                        # Ins√©rer l'auteur avec gestion automatique des doublons
                        result = session.execute(
                            text("""
                                INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) 
                                VALUES (:id_auteur, :nom, NULL, NULL)
                                ON CONFLICT (id_auteur) DO NOTHING
                                RETURNING id_auteur
                            """),
                            {
                                "id_auteur": author_key,
                                "nom": author_name
                            }
                        )
                        
                        # V√©rifier si l'insertion a r√©ussi (retourne une ligne) ou si c'√©tait un doublon
                        if result.fetchone() is not None:
                            authors_inserted += 1
                        else:
                            authors_skipped += 1
                        
                        pbar.set_postfix({
                            "Ins√©r√©s": f"{authors_inserted:,}", 
                            "Ignor√©s": f"{authors_skipped:,}"
                        })
                        
                        # Commit p√©riodique
                        if (authors_inserted + authors_skipped) % 1000 == 0:
                            session.commit()
                            
                    except Exception as e:
                        print(f"\n‚ùå Erreur lors de l'insertion de {author_key}: {e}")
                        session.rollback()
                    
                    # Mettre √† jour la barre de progression
                    pbar.update(1)
        
        # Commit final
        session.commit()
        session.close()
        
        print(f"\n=== R√âSULTATS DE L'INSERTION ===")
        print(f"Auteurs ins√©r√©s     : {authors_inserted:,}")
        print(f"Auteurs ignor√©s     : {authors_skipped:,} (d√©j√† existants)")
        print(f"Total trait√©        : {authors_inserted + authors_skipped:,}")
        print("Insertion termin√©e avec succ√®s !")
        
    except Exception as e:
        print(f"Erreur : {e}")
        if 'session' in locals():
            session.rollback()
            session.close()

def insert_authors_batch():
    """
    Version optimis√©e avec insertion par batch pour de meilleures performances avec SQLAlchemy.
    """
    csv_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\extracted_authors.csv")
    
    if not csv_file.exists():
        print(f"Le fichier CSV {csv_file} n'existe pas.")
        return
    
    # Param√®tres de connexion
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    batch_size = 1000  # Ins√©rer par batch de 1000
    
    try:
        # Cr√©er l'engine SQLAlchemy
        engine = create_engine(DATABASE_URL)
        
        # Compter le nombre total d'auteurs
        print("üìä Analyse du fichier...")
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            total_authors = sum(1 for line in csvfile) - 1  # -1 pour l'en-t√™te
        
        print(f"üìà {total_authors:,} auteurs √† traiter par batch de {batch_size}")
        
        batch_data = []
        total_inserted = 0
        
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            # Barre de progression pour l'insertion par batch
            with tqdm(total=total_authors, desc="‚ö° Insertion batch", unit="auteurs",
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
                
                for row in reader:
                    batch_data.append({
                        'id_auteur': row['author_key'],
                        'nom': row['author_name'],
                        'date_naissance': None,
                        'biographie': None
                    })
                    
                    if len(batch_data) >= batch_size:
                        # Ins√©rer le batch avec SQLAlchemy
                        with engine.connect() as conn:
                            conn.execute(
                                text("""
                                    INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) 
                                    VALUES (:id_auteur, :nom, :date_naissance, :biographie)
                                    ON CONFLICT (id_auteur) DO NOTHING
                                """),
                                batch_data
                            )
                            conn.commit()
                        
                        total_inserted += len(batch_data)
                        pbar.update(len(batch_data))
                        pbar.set_postfix({
                            "Batch": f"{total_inserted//batch_size:,}",
                            "Trait√©s": f"{total_inserted:,}"
                        })
                        
                        batch_data = []
                
                # Ins√©rer le dernier batch
                if batch_data:
                    with engine.connect() as conn:
                        conn.execute(
                            text("""
                                INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) 
                                VALUES (:id_auteur, :nom, :date_naissance, :biographie)
                                ON CONFLICT (id_auteur) DO NOTHING
                            """),
                            batch_data
                        )
                        conn.commit()
                    total_inserted += len(batch_data)
                    pbar.update(len(batch_data))
                    pbar.set_postfix({
                        "Batch": "Final",
                        "Trait√©s": f"{total_inserted:,}"
                    })
        
        engine.dispose()
        print(f"\nInsertion termin√©e : {total_inserted:,} auteurs trait√©s")
        
    except Exception as e:
        print(f"Erreur : {e}")

def insert_authors_pandas():
    """
    Version ultra-optimis√©e avec Pandas et SQLAlchemy pour tr√®s gros volumes.
    V√©rifie d'abord les auteurs d√©j√† pr√©sents et ne traite que les nouveaux.
    """
    csv_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\extracted_authors.csv")
    
    if not csv_file.exists():
        print(f"Le fichier CSV {csv_file} n'existe pas.")
        return
    
    # Param√®tres de connexion
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    try:
        # Cr√©er l'engine SQLAlchemy
        engine = create_engine(DATABASE_URL)
        
        print("=== INSERTION ULTRA-OPTIMIS√âE AVEC PANDAS ===")
        print(f"üìä Lecture du fichier CSV : {csv_file}")
        
        # Lire le CSV avec Pandas et nettoyer les donn√©es
        print("üîç Chargement et nettoyage des donn√©es...")
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['author_name'])  # Supprimer les noms vides
        df = df.drop_duplicates(subset=['author_key'])  # Supprimer les doublons dans le CSV
        
        print(f"üìà Donn√©es du CSV : {len(df):,} auteurs uniques")
        
        # √âTAPE 1 : R√©cup√©rer tous les id_auteur d√©j√† pr√©sents en base
        print("üîç V√©rification des auteurs d√©j√† pr√©sents en base...")
        with engine.connect() as conn:
            # R√©cup√©rer tous les id_auteur existants en une seule requ√™te
            existing_authors_result = conn.execute(
                text("SELECT id_auteur FROM auteur")
            )
            existing_authors = {row[0] for row in existing_authors_result.fetchall()}
        
        print(f"üìã Auteurs d√©j√† en base : {len(existing_authors):,}")
        
        # √âTAPE 2 : Filtrer pour ne garder que les nouveaux auteurs
        print("üîÑ Filtrage des nouveaux auteurs...")
        df_new = df[~df['author_key'].isin(existing_authors)].copy()
        
        print(f"‚úÖ Nouveaux auteurs √† ins√©rer : {len(df_new):,}")
        print(f"‚ö†Ô∏è  Auteurs d√©j√† pr√©sents (ignor√©s) : {len(df) - len(df_new):,}")
        
        # Si aucun nouvel auteur, on s'arr√™te ici
        if len(df_new) == 0:
            print("üéâ Aucun nouvel auteur √† ins√©rer ! Tous sont d√©j√† en base.")
            engine.dispose()
            return
        
        # √âTAPE 3 : Pr√©parer les donn√©es pour insertion
        df_insert = pd.DataFrame({
            'id_auteur': df_new['author_key'],
            'nom': df_new['author_name'],
            'date_naissance': None,
            'biographie': None
        })
        
        # √âTAPE 4 : Insertion par chunks (plus rapide pour gros volumes)
        chunk_size = 1000
        total_chunks = (len(df_insert) + chunk_size - 1) // chunk_size
        authors_inserted = 0
        
        print(f"‚ö° Insertion par chunks de {chunk_size}...")
        
        with tqdm(total=len(df_insert), desc="üöÄ Insertion nouveaux", unit="auteurs",
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            
            for i in range(0, len(df_insert), chunk_size):
                chunk = df_insert.iloc[i:i+chunk_size]
                
                # Convertir le chunk en liste de dictionnaires
                chunk_data = chunk.to_dict('records')
                
                with engine.connect() as conn:
                    # Insertion directe (pas de ON CONFLICT car on a d√©j√† filtr√©)
                    conn.execute(
                        text("""
                            INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) 
                            VALUES (:id_auteur, :nom, :date_naissance, :biographie)
                        """),
                        chunk_data
                    )
                    conn.commit()
                
                authors_inserted += len(chunk)
                
                # Mettre √† jour la barre de progression
                pbar.update(len(chunk))
                pbar.set_postfix({
                    "‚úÖ Ins√©r√©s": f"{authors_inserted:,}",
                    "Chunk": f"{(i//chunk_size)+1}/{total_chunks}"
                })
        
        engine.dispose()
        
        print(f"\n=== R√âSULTATS PANDAS INSERTION ===")
        print(f"üìä Total dans CSV         : {len(df):,}")
        print(f"‚ö†Ô∏è  D√©j√† en base (ignor√©s) : {len(df) - len(df_new):,}")
        print(f"‚úÖ Nouveaux ins√©r√©s       : {authors_inserted:,}")
        print(f"üéØ Efficacit√©             : {(authors_inserted/len(df)*100):.1f}% de nouveaux")
        
    except Exception as e:
        print(f"‚ùå Erreur : {e}")

def test_extraction_100k():
    """
    FONCTION DE TEST : Extrait et ins√®re seulement 100 000 lignes pour tester.
    """
    # Chemins des fichiers
    input_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\ol_dump_authors_2025-05-31.txt.gz")
    output_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\test_authors_100k.csv")
    
    print("=== TEST : EXTRACTION DE 100 000 LIGNES ===")
    print(f"Lecture du fichier : {input_file}")
    print(f"Sauvegarde vers : {output_file}")
    
    # Compteurs pour le suivi
    total_lines = 0
    extracted_authors = 0
    errors = 0
    
    # Liste pour stocker les auteurs extraits
    authors_data = []
    
    try:
        # Ouvrir le fichier compress√©
        with gzip.open(input_file, 'rt', encoding='utf-8') as file:
            for line_num, line in enumerate(file, 1):
                total_lines += 1
                
                # Afficher le progr√®s tous les 10000 lignes
                if line_num % 10000 == 0:
                    print(f"Traitement ligne {line_num:,}... ({extracted_authors:,} auteurs trouv√©s)")
                
                try:
                    # S√©parer les colonnes (tab-separated)
                    parts = line.strip().split('\t')
                    
                    if len(parts) >= 5:
                        type_field = parts[0]
                        author_key = parts[1]
                        json_data = parts[4]
                        
                        # V√©rifier que c'est bien un auteur
                        if type_field == "/type/author" and author_key.startswith("/authors/"):
                            try:
                                # Parser le JSON
                                author_data = json.loads(json_data)
                                
                                # Extraire le nom de l'auteur
                                if "name" in author_data:
                                    author_name = author_data["name"]
                                    
                                    # Nettoyer les donn√©es
                                    author_key_clean = author_key.replace("/authors/", "")
                                    author_name_clean = str(author_name).strip()
                                    
                                    # Ajouter √† la liste
                                    authors_data.append({
                                        'author_key': author_key_clean,
                                        'author_name': author_name_clean,
                                        'full_key': author_key
                                    })
                                    
                                    extracted_authors += 1
                                    
                            except json.JSONDecodeError:
                                errors += 1
                                continue
                                
                except Exception as e:
                    errors += 1
                    continue
                    
                # LIMITE POUR LE TEST : 100 000 lignes
                if line_num >= 100000:
                    print(f"‚úÖ Test termin√© apr√®s {line_num:,} lignes")
                    break
    
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier : {e}")
        return None
    
    # Sauvegarder dans un fichier CSV
    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['author_key', 'author_name', 'full_key']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            # √âcrire l'en-t√™te
            writer.writeheader()
            
            # √âcrire les donn√©es
            for author in authors_data:
                writer.writerow(author)
        
        print(f"\n=== R√âSULTATS DU TEST ===")
        print(f"Lignes trait√©es      : {total_lines:,}")
        print(f"Auteurs extraits     : {extracted_authors:,}")
        print(f"Erreurs              : {errors:,}")
        print(f"Taux de r√©ussite     : {(extracted_authors/(total_lines-errors)*100):.2f}%")
        print(f"Fichier test cr√©√©    : {output_file}")
        
        # Afficher quelques exemples
        print(f"\n=== EXEMPLES D'AUTEURS EXTRAITS ===")
        for i, author in enumerate(authors_data[:10]):
            print(f"{i+1:2d}. {author['author_key']:15s} | {author['author_name']}")
        
        return output_file
            
    except Exception as e:
        print(f"Erreur lors de la sauvegarde : {e}")
        return None

def test_insertion_database():
    """
    FONCTION DE TEST : Ins√®re les auteurs du fichier test dans la base de donn√©es.
    """
    csv_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\test_authors_100k.csv")
    
    if not csv_file.exists():
        print(f"‚ùå Le fichier test {csv_file} n'existe pas.")
        print("Ex√©cutez d'abord test_extraction_100k() pour cr√©er le fichier.")
        return
    
    # Param√®tres de connexion
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    print("=== TEST : INSERTION EN BASE DE DONN√âES ===")
    print(f"Lecture du fichier test : {csv_file}")
    
    try:
        # Cr√©er l'engine SQLAlchemy
        engine = create_engine(DATABASE_URL)
        
        print("‚úÖ Connexion √† la base de donn√©es...")
        
        # Test de connexion
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1")).scalar()
            print(f"‚úÖ Test de connexion r√©ussi : {result}")
        
        # Lire le fichier CSV
        authors_inserted = 0
        authors_skipped = 0
        
        # Compter le nombre total d'auteurs pour le test
        print("üìä Comptage des auteurs...")
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            total_authors = sum(1 for line in csvfile) - 1  # -1 pour l'en-t√™te
        
        print(f"üìà {total_authors:,} auteurs √† traiter (test)")
        
        with open(csv_file, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            # Barre de progression pour le test
            with tqdm(total=total_authors, desc="üß™ Test insertion", unit="auteurs",
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
                
                with engine.connect() as conn:
                    for row in reader:
                        author_key = row['author_key']
                        author_name = row['author_name']
                        
                        try:
                            # V√©rifier si l'auteur existe d√©j√†
                            result = conn.execute(
                                text("SELECT COUNT(*) FROM auteur WHERE id_auteur = :id_auteur"),
                                {"id_auteur": author_key}
                            ).scalar()
                            
                            # Ins√©rer l'auteur avec gestion automatique des doublons
                            result = conn.execute(
                                text("""
                                    INSERT INTO auteur (id_auteur, nom, date_naissance, biographie) 
                                    VALUES (:id_auteur, :nom, NULL, NULL)
                                    ON CONFLICT (id_auteur) DO NOTHING
                                    RETURNING id_auteur
                                """),
                                {
                                    "id_auteur": author_key,
                                    "nom": author_name
                                }
                            )
                            
                            # V√©rifier si l'insertion a r√©ussi
                            if result.fetchone() is not None:
                                authors_inserted += 1
                            else:
                                authors_skipped += 1
                            
                            pbar.set_postfix({
                                "‚úÖ Ins√©r√©s": f"{authors_inserted:,}",
                                "‚ö†Ô∏è Ignor√©s": f"{authors_skipped:,}"
                            })
                            
                            # Commit p√©riodique
                            if (authors_inserted + authors_skipped) % 500 == 0:
                                conn.commit()
                                
                        except Exception as e:
                            print(f"\n‚ùå Erreur lors de l'insertion de {author_key}: {e}")
                            continue
                        
                        # Mettre √† jour la barre
                        pbar.update(1)
                    
                    # Commit final
                    conn.commit()
        
        engine.dispose()
        
        print(f"\n=== R√âSULTATS DE L'INSERTION TEST ===")
        print(f"‚úÖ Auteurs ins√©r√©s     : {authors_inserted:,}")
        print(f"‚ö†Ô∏è  Auteurs ignor√©s     : {authors_skipped:,} (d√©j√† existants)")
        print(f"üìä Total trait√©        : {authors_inserted + authors_skipped:,}")
        
        if authors_inserted > 0:
            print("üéâ Test d'insertion r√©ussi ! Le syst√®me fonctionne correctement.")
        else:
            print("‚ö†Ô∏è  Aucun nouvel auteur ins√©r√©. V√©rifiez si les donn√©es existent d√©j√†.")
        
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        print("üí° V√©rifiez que PostgreSQL est d√©marr√© et que la base 'databook' existe.")

def run_full_test():
    """
    Ex√©cute le test complet : extraction + insertion.
    """
    print("üöÄ D√âMARRAGE DU TEST COMPLET")
    print("=" * 50)
    
    # √âtape 1 : Extraction
    print("1Ô∏è‚É£ PHASE 1 : Extraction des auteurs (100k lignes)")
    csv_file = test_extraction_100k()
    
    if csv_file is None:
        print("‚ùå √âchec de l'extraction. Test arr√™t√©.")
        return
    
    print("\n" + "=" * 50)
    
    # √âtape 2 : Insertion
    print("2Ô∏è‚É£ PHASE 2 : Insertion en base de donn√©es")
    test_insertion_database()
    
    print("\n" + "=" * 50)
    print("üèÅ TEST COMPLET TERMIN√â")

def ensure_unique_constraint():
    """
    S'assure que la contrainte UNIQUE existe sur la colonne id_auteur.
    """
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    try:
        engine = create_engine(DATABASE_URL)
        
        with engine.connect() as conn:
            # V√©rifier si la contrainte unique existe d√©j√†
            result = conn.execute(text("""
                SELECT constraint_name 
                FROM information_schema.table_constraints 
                WHERE table_name = 'auteur' 
                AND constraint_type = 'UNIQUE' 
                AND constraint_name LIKE '%id_auteur%'
            """)).fetchall()
            
            if not result:
                print("üîß Cr√©ation de la contrainte UNIQUE sur id_auteur...")
                # Cr√©er la contrainte unique si elle n'existe pas
                conn.execute(text("""
                    ALTER TABLE auteur 
                    ADD CONSTRAINT auteur_id_auteur_unique 
                    UNIQUE (id_auteur)
                """))
                conn.commit()
                print("‚úÖ Contrainte UNIQUE cr√©√©e avec succ√®s")
            else:
                print("‚úÖ Contrainte UNIQUE sur id_auteur d√©j√† existante")
        
        engine.dispose()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la v√©rification/cr√©ation de la contrainte : {e}")
        print("üí° La contrainte sera g√©r√©e par ON CONFLICT DO NOTHING")

def update_birth_death_dates_from_csv():
    """
    Met √† jour les dates de naissance et de mort dans la table auteur
    depuis le CSV 'auteurs_avec_date_naissance.csv'
    """
    csv_file = Path(r"C:\Users\dd758\Formation_IA_Greta\Projet_possible certif\Livre_analyse\data_book\databook\data\fichier_openlibrary\auteurs_avec_date_naissance.csv")
    
    if not csv_file.exists():
        print(f"‚ùå Le fichier CSV {csv_file} n'existe pas.")
        print("Veuillez d'abord ex√©cuter l'extraction des auteurs avec dates de naissance.")
        return
    
    # Param√®tres de connexion
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    try:
        # Cr√©er l'engine SQLAlchemy
        engine = create_engine(DATABASE_URL)
        
        print("=== MISE √Ä JOUR DES DATES DE NAISSANCE ET MORT ===")
        print(f"üìÇ Lecture du fichier CSV : {csv_file}")
        
        # Lire le CSV avec Pandas
        print("üìä Chargement des donn√©es...")
        df = pd.read_csv(csv_file)
        
        print(f"üìà Donn√©es du CSV : {len(df):,} auteurs")
        
        # V√©rifier les colonnes n√©cessaires
        required_columns = ['author_id', 'birth_date', 'death_date', 'birth_year', 'death_year']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"‚ùå Colonnes manquantes dans le CSV : {missing_columns}")
            return
        
        # Statistiques des donn√©es disponibles
        birth_dates_count = df['birth_date'].notna().sum()
        death_dates_count = df['death_date'].notna().sum()
        birth_years_count = df['birth_year'].notna().sum()
        death_years_count = df['death_year'].notna().sum()
        
        print(f"üìã Donn√©es disponibles :")
        print(f"   ‚Ä¢ Dates de naissance : {birth_dates_count:,}")
        print(f"   ‚Ä¢ Dates de mort      : {death_dates_count:,}")
        print(f"   ‚Ä¢ Ann√©es naissance   : {birth_years_count:,}")
        print(f"   ‚Ä¢ Ann√©es mort        : {death_years_count:,}")
        
        # V√©rifier d'abord si la colonne date_mort existe
        print("üîç V√©rification de la structure de la table...")
        with engine.connect() as conn:
            # V√©rifier les colonnes existantes
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'auteur' 
                AND column_name IN ('date_naissance', 'date_mort')
            """)).fetchall()
            
            existing_columns = [row[0] for row in result]
            
            # Ajouter la colonne date_mort si elle n'existe pas
            if 'date_mort' not in existing_columns:
                print("üìù Ajout de la colonne date_mort...")
                conn.execute(text("""
                    ALTER TABLE auteur 
                    ADD COLUMN date_mort VARCHAR(100) NULL
                """))
                conn.commit()
                print("‚úÖ Colonne date_mort ajout√©e")
            else:
                print("‚úÖ Colonne date_mort d√©j√† pr√©sente")
        
        # Pr√©parer les donn√©es pour la mise √† jour
        authors_to_update = []
        
        for _, row in df.iterrows():
            author_id = row['author_id']
            birth_date = row['birth_date'] if pd.notna(row['birth_date']) else None
            death_date = row['death_date'] if pd.notna(row['death_date']) else None
            
            # Ne traiter que les auteurs avec au moins une date
            if birth_date or death_date:
                authors_to_update.append({
                    'author_id': author_id,
                    'birth_date': birth_date,
                    'death_date': death_date
                })
        
        print(f"üéØ Auteurs √† mettre √† jour : {len(authors_to_update):,}")
        
        if len(authors_to_update) == 0:
            print("‚ö†Ô∏è Aucun auteur avec dates √† mettre √† jour")
            return
        
        # Phase 1 : V√©rifier les correspondances avec la base
        print("üîç Phase 1 : V√©rification des correspondances...")
        
        found_authors = 0
        not_found_authors = 0
        updates_data = []
        
        with engine.connect() as conn:
            with tqdm(total=len(authors_to_update), desc="üîç V√©rification", unit="auteurs") as pbar:
                for author_data in authors_to_update:
                    author_id = author_data['author_id']
                    
                    # Chercher l'auteur dans la base par cl√© OpenLibrary
                    # Essayer diff√©rentes variantes de recherche
                    search_patterns = [
                        author_id,  # Cl√© exacte
                        f"/{author_id}",  # Avec slash
                        f"/authors/{author_id}",  # Cl√© compl√®te
                        f"%{author_id}%"  # Pattern g√©n√©ral
                    ]
                    
                    author_found = False
                    db_id = None
                    
                    for pattern in search_patterns:
                        result = conn.execute(text("""
                            SELECT id_auteur 
                            FROM auteur 
                            WHERE id_auteur ILIKE :pattern 
                            LIMIT 1
                        """), {"pattern": pattern}).fetchone()
                        
                        if result:
                            db_id = result[0]
                            author_found = True
                            break
                    
                    if author_found:
                        found_authors += 1
                        updates_data.append({
                            'db_id': db_id,
                            'birth_date': author_data['birth_date'],
                            'death_date': author_data['death_date']
                        })
                    else:
                        not_found_authors += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        "Trouv√©s": found_authors,
                        "Non trouv√©s": not_found_authors
                    })
        
        print(f"üìä R√©sultats de la v√©rification :")
        print(f"   ‚úÖ Auteurs trouv√©s     : {found_authors:,}")
        print(f"   ‚ùå Auteurs non trouv√©s : {not_found_authors:,}")
        
        if found_authors == 0:
            print("‚ö†Ô∏è Aucun auteur trouv√© dans la base pour mise √† jour")
            return
        
        # Phase 2 : Mise √† jour des dates
        print("üîÑ Phase 2 : Mise √† jour des dates...")
        
        birth_updates = 0
        death_updates = 0
        
        with engine.connect() as conn:
            with tqdm(total=len(updates_data), desc="üîÑ Mise √† jour", unit="auteurs") as pbar:
                for update_data in updates_data:
                    db_id = update_data['db_id']
                    birth_date = update_data['birth_date']
                    death_date = update_data['death_date']
                    
                    try:
                        # Construire la requ√™te de mise √† jour dynamiquement
                        set_clauses = []
                        params = {'db_id': db_id}
                        
                        if birth_date:
                            set_clauses.append("date_naissance = :birth_date")
                            params['birth_date'] = birth_date
                            birth_updates += 1
                        
                        if death_date:
                            set_clauses.append("date_mort = :death_date")
                            params['death_date'] = death_date
                            death_updates += 1
                        
                        if set_clauses:
                            update_sql = f"""
                                UPDATE auteur 
                                SET {', '.join(set_clauses)}
                                WHERE id_auteur = :db_id
                            """
                            
                            conn.execute(text(update_sql), params)
                        
                        # Commit p√©riodique
                        if pbar.n % 100 == 0:
                            conn.commit()
                    
                    except Exception as e:
                        print(f"\n‚ùå Erreur lors de la mise √† jour de {db_id}: {e}")
                        continue
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        "Naissances": birth_updates,
                        "Morts": death_updates
                    })
            
            # Commit final
            conn.commit()
        
        engine.dispose()
        
        print(f"\n=== R√âSULTATS DE LA MISE √Ä JOUR ===")
        print(f"‚úÖ Auteurs trait√©s         : {found_authors:,}")
        print(f"üìÖ Dates de naissance mises √† jour : {birth_updates:,}")
        print(f"‚ö∞Ô∏è  Dates de mort mises √† jour     : {death_updates:,}")
        print(f"üéØ Taux de correspondance  : {(found_authors/len(authors_to_update)*100):.1f}%")
        print("üéâ Mise √† jour termin√©e avec succ√®s !")
        
        # Afficher quelques exemples de mises √† jour
        print(f"\nüìã V√âRIFICATION POST-MISE √Ä JOUR :")
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT id_auteur, nom, date_naissance, date_mort
                FROM auteur 
                WHERE date_naissance IS NOT NULL OR date_mort IS NOT NULL
                ORDER BY date_naissance DESC
                LIMIT 5
            """)).fetchall()
            
            print("Quelques exemples d'auteurs avec dates :")
            for row in result:
                id_auteur, nom, date_naissance, date_mort = row
                birth_info = date_naissance if date_naissance else "Non renseign√©"
                death_info = date_mort if date_mort else "Vivant/Non renseign√©"
                print(f"   ‚Ä¢ {nom:<30} | N√©: {birth_info} | Mort: {death_info}")
        
    except Exception as e:
        print(f"‚ùå Erreur : {e}")

def verify_dates_update():
    """
    V√©rifie les dates mises √† jour dans la base de donn√©es
    """
    DATABASE_URL = "postgresql://postgres:postgres@localhost:5432/databook"
    
    try:
        engine = create_engine(DATABASE_URL)
        
        print("=== V√âRIFICATION DES DATES DANS LA BASE ===")
        
        with engine.connect() as conn:
            # Statistiques g√©n√©rales
            result = conn.execute(text("""
                SELECT 
                    COUNT(*) as total_auteurs,
                    COUNT(date_naissance) as avec_date_naissance,
                    COUNT(date_mort) as avec_date_mort,
                    COUNT(CASE WHEN date_naissance IS NOT NULL AND date_mort IS NOT NULL THEN 1 END) as avec_les_deux
                FROM auteur
            """)).fetchone()
            
            total, avec_naissance, avec_mort, avec_les_deux = result
            
            print(f"üìä STATISTIQUES DE LA TABLE AUTEUR :")
            print(f"   üë• Total auteurs           : {total:,}")
            print(f"   üéÇ Avec date de naissance  : {avec_naissance:,} ({(avec_naissance/total*100):.1f}%)")
            print(f"   ‚ö∞Ô∏è  Avec date de mort       : {avec_mort:,} ({(avec_mort/total*100):.1f}%)")
            print(f"   üìã Avec les deux dates     : {avec_les_deux:,} ({(avec_les_deux/total*100):.1f}%)")
            
            # Exemples d'auteurs avec dates compl√®tes
            print(f"\nüìö EXEMPLES D'AUTEURS AVEC DATES COMPL√àTES :")
            result = conn.execute(text("""
                SELECT nom, date_naissance, date_mort
                FROM auteur 
                WHERE date_naissance IS NOT NULL AND date_mort IS NOT NULL
                ORDER BY RANDOM()
                LIMIT 10
            """)).fetchall()
            
            for i, (nom, naissance, mort) in enumerate(result, 1):
                print(f"   {i:2d}. {nom:<35} | {naissance} - {mort}")
            
            # Auteurs les plus anciens
            print(f"\nüë¥ AUTEURS LES PLUS ANCIENS (avec date de naissance) :")
            result = conn.execute(text("""
                SELECT nom, date_naissance, date_mort
                FROM auteur 
                WHERE date_naissance IS NOT NULL
                AND date_naissance ~ '^[0-9]+'  -- Commence par un chiffre
                ORDER BY CAST(SUBSTRING(date_naissance FROM '^[0-9]+') AS INTEGER)
                LIMIT 5
            """)).fetchall()
            
            for i, (nom, naissance, mort) in enumerate(result, 1):
                mort_info = f" (mort en {mort})" if mort else " (date de mort inconnue)"
                print(f"   {i}. {nom} - n√© en {naissance}{mort_info}")
        
        engine.dispose()
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification : {e}")

if __name__ == "__main__":
    print("=== EXTRACTION ET INSERTION DES AUTEURS OPENLIBRARY ===")
    print("Choisissez une option :")
    print("1. Extraire les auteurs du fichier dump")
    print("2. Cr√©er le fichier SQL")
    print("3. Ins√©rer dans la base de donn√©es (SQLAlchemy standard)")
    print("4. Ins√©rer dans la base de donn√©es (SQLAlchemy batch)")
    print("5. Ins√©rer dans la base de donn√©es (Pandas ultra-rapide)")
    print("6. Tout faire (extraction + insertion)")
    print("7. üß™ TEST : Extraction 100k lignes")
    print("8. üß™ TEST : Insertion en base")
    print("9. üß™ TEST COMPLET (extraction + insertion)")
    print("10. üîß V√©rifier/Cr√©er contrainte UNIQUE")
    print("11. üîß Mettre √† jour les dates de naissance et de mort")
    print("12. ÔøΩÔøΩ V√©rifier les dates mises √† jour")
    
    choice = input("Votre choix (1-12) : ").strip()
    
    if choice == "1":
        extract_authors_from_dump()
    elif choice == "2":
        create_sql_insert_file()
    elif choice == "3":
        insert_authors_to_database()
    elif choice == "4":
        insert_authors_batch()
    elif choice == "5":
        insert_authors_pandas()
    elif choice == "6":
        print("1. Extraction des donn√©es...")
        extract_authors_from_dump()
        print("\n2. Insertion dans la base...")
        insert_authors_batch()
    elif choice == "7":
        test_extraction_100k()
    elif choice == "8":
        test_insertion_database()
    elif choice == "9":
        run_full_test()
    elif choice == "10":
        ensure_unique_constraint()
    elif choice == "11":
        update_birth_death_dates_from_csv()
    elif choice == "12":
        verify_dates_update()
    else:
        print("Choix invalide. Ex√©cution de l'extraction par d√©faut...")
        extract_authors_from_dump() 