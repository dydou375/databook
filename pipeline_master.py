#!/usr/bin/env python3
"""
PIPELINE MASTER DATABOOK
========================

Script principal qui orchestre toute la cha√Æne de r√©cup√©ration et traitement des donn√©es :
1. üì° R√©cup√©ration donn√©es API (Google Books, OpenLibrary)
2. üï∑Ô∏è Scrapping Babelio (critiques et notes)
3. üìä Traitement fichiers CSV (nettoyage, formatage)
4. üóÑÔ∏è Import PostgreSQL (tables relationnelles)
5. üçÉ Import MongoDB (documents JSON)
6. ‚úÖ Validation et v√©rification

Author: Assistant IA
Date: 2025-01-27
"""

import os
import sys
import time
import json
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'pipeline_master_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class PipelineMaster:
    """Gestionnaire principal du pipeline DataBook"""
    
    def __init__(self):
        self.workspace = Path(__file__).parent
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Chemins des dossiers
        self.paths = {
            'scripts_api': self.workspace / "scripts" / "api",
            'scripts_scrapping': self.workspace / "scripts" / "scrapping", 
            'scripts_bdd': self.workspace / "bdd",
            'data_json': self.workspace / "data" / "livre_json",
            'data_csv': self.workspace / "data",
            'deploiement': self.workspace / "deploiement_base_local"
        }
        
        # √âtat du pipeline
        self.etat = {
            'etape_actuelle': 0,
            'total_etapes': 8,
            'debut': datetime.now(),
            'logs': [],
            'erreurs': [],
            'resultats': {}
        }
        
        # Configuration par d√©faut
        self.config = {
            'api': {
                'max_livres_par_categorie': 1000,
                'categories_actives': 20,
                'delai_requetes': 2
            },
            'scrapping': {
                'max_livres_babelio': 500,
                'delai_scrapping': 3,
                'fichier_csv_source': None
            },
            'bdd': {
                'postgresql_url': "postgresql://postgres:postgres@localhost:5432/databook",
                'mongodb_url': "mongodb://localhost:27017/",
                'schema_postgres': "test",
                'batch_size': 1000
            }
        }
        
        logger.info("üöÄ PIPELINE MASTER DATABOOK INITIALIS√â")
        logger.info(f"üìÇ Workspace: {self.workspace}")
        logger.info(f"üïê Timestamp: {self.timestamp}")

    def afficher_menu_principal(self) -> str:
        """Affiche le menu principal et retourne le choix"""
        print("\n" + "="*60)
        print("üéØ PIPELINE MASTER DATABOOK")
        print("="*60)
        print("Choisissez le mode d'ex√©cution :")
        print()
        print("1. üöÄ PIPELINE COMPLET (toutes les √©tapes)")
        print("2. üì° R√©cup√©ration API uniquement")
        print("3. üï∑Ô∏è Scrapping Babelio uniquement") 
        print("4. üìä Traitement CSV uniquement")
        print("5. üóÑÔ∏è Import BDD uniquement")
        print("6. ‚öôÔ∏è Configuration personnalis√©e")
        print("7. üìã Status et diagnostics")
        print("8. üßπ Nettoyage des donn√©es temporaires")
        print("9. ‚ùå Quitter")
        print()
        
        return input("Votre choix (1-9): ").strip()

    def configurer_pipeline(self):
        """Interface de configuration personnalis√©e"""
        print("\n‚öôÔ∏è CONFIGURATION PERSONNALIS√âE")
        print("-" * 40)
        
        # Configuration API
        print("\nüì° R√âCUP√âRATION API")
        self.config['api']['max_livres_par_categorie'] = int(input(
            f"Nombre max de livres par cat√©gorie [{self.config['api']['max_livres_par_categorie']}]: "
        ) or self.config['api']['max_livres_par_categorie'])
        
        # Configuration scrapping
        print("\nüï∑Ô∏è SCRAPPING BABELIO")
        self.config['scrapping']['max_livres_babelio'] = int(input(
            f"Nombre max de livres √† scrapper [{self.config['scrapping']['max_livres_babelio']}]: "
        ) or self.config['scrapping']['max_livres_babelio'])
        
        # Configuration BDD
        print("\nüóÑÔ∏è BASES DE DONN√âES")
        nouveau_schema = input(f"Nom du sch√©ma PostgreSQL [{self.config['bdd']['schema_postgres']}]: ").strip()
        if nouveau_schema:
            self.config['bdd']['schema_postgres'] = nouveau_schema
            
        logger.info("‚úÖ Configuration mise √† jour")

    def executer_etape_api(self) -> bool:
        """√âtape 1: R√©cup√©ration donn√©es API"""
        self.etat['etape_actuelle'] = 1
        logger.info("üì° √âTAPE 1/8: R√©cup√©ration donn√©es API")
        
        try:
            script_api = self.paths['scripts_api'] / "recup√©ration_api_livre_amelioree.py"
            
            if not script_api.exists():
                raise FileNotFoundError(f"Script API non trouv√©: {script_api}")
            
            # Ex√©cuter le script de r√©cup√©ration API
            logger.info("üîÑ Lancement r√©cup√©ration Google Books + OpenLibrary...")
            
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.workspace)
            
            result = subprocess.run([
                sys.executable, str(script_api)
            ], cwd=str(self.paths['scripts_api']), 
               capture_output=True, text=True, env=env, timeout=3600)
            
            if result.returncode == 0:
                logger.info("‚úÖ R√©cup√©ration API termin√©e avec succ√®s")
                
                # Compter les fichiers JSON cr√©√©s
                json_dir = self.paths['data_json'] / "livres_json_ameliore"
                if json_dir.exists():
                    fichiers_json = list(json_dir.glob("*.json"))
                    total_livres = 0
                    
                    for fichier in fichiers_json:
                        try:
                            with open(fichier, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, list):
                                    total_livres += len(data)
                                else:
                                    total_livres += 1
                        except:
                            pass
                    
                    self.etat['resultats']['api'] = {
                        'fichiers_json': len(fichiers_json),
                        'total_livres': total_livres
                    }
                    logger.info(f"üìä R√©sultats API: {len(fichiers_json)} fichiers, ~{total_livres:,} livres")
                
                return True
            else:
                logger.error(f"‚ùå Erreur r√©cup√©ration API: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Exception √©tape API: {e}")
            return False

    def executer_etape_scrapping(self) -> bool:
        """√âtape 2: Scrapping Babelio"""
        self.etat['etape_actuelle'] = 2
        logger.info("üï∑Ô∏è √âTAPE 2/8: Scrapping Babelio")
        
        try:
            script_babelio = self.paths['scripts_scrapping'] / "babelio_scraper_final.py"
            
            if not script_babelio.exists():
                raise FileNotFoundError(f"Script Babelio non trouv√©: {script_babelio}")
            
            # Chercher un fichier CSV source pour les ISBN
            csv_source = None
            if self.config['scrapping']['fichier_csv_source']:
                csv_source = self.config['scrapping']['fichier_csv_source']
            else:
                # Chercher automatiquement
                csv_files = list(self.workspace.rglob("*.csv"))
                csv_files = [f for f in csv_files if f.stat().st_size > 1024*1024]  # > 1MB
                
                if csv_files:
                    print("\nüìã Fichiers CSV disponibles:")
                    for i, csv_file in enumerate(csv_files[:5], 1):
                        size_mb = csv_file.stat().st_size / (1024*1024)
                        print(f"   {i}. {csv_file.name} ({size_mb:.1f} MB)")
                    
                    choix = input(f"Choisir fichier (1-{min(5, len(csv_files))}) ou Enter pour passer: ").strip()
                    if choix.isdigit() and 1 <= int(choix) <= len(csv_files):
                        csv_source = str(csv_files[int(choix)-1])
            
            if csv_source:
                logger.info(f"üîÑ Scrapping Babelio depuis: {Path(csv_source).name}")
                # Note: Le script Babelio est interactif, on le lance en mode guid√©
                logger.info("üí° Script Babelio pr√™t - lancement manuel requis pour configuration interactive")
                self.etat['resultats']['scrapping'] = {'status': 'pr√™t', 'source': csv_source}
            else:
                logger.info("‚è≠Ô∏è Scrapping Babelio ignor√© - aucun fichier CSV source")
                self.etat['resultats']['scrapping'] = {'status': 'ignor√©'}
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Exception √©tape scrapping: {e}")
            return False

    def executer_etape_nettoyage_csv(self) -> bool:
        """√âtape 3: Nettoyage et formatage CSV"""
        self.etat['etape_actuelle'] = 3
        logger.info("üìä √âTAPE 3/8: Nettoyage et formatage CSV")
        
        try:
            # Chercher des fichiers CSV √† nettoyer
            csv_bruts = []
            for pattern in ["livres_openlibrary_*.csv", "*.csv"]:
                csv_bruts.extend(self.workspace.rglob(pattern))
            
            # Filtrer par taille (> 10MB)
            csv_bruts = [f for f in csv_bruts if f.stat().st_size > 10*1024*1024]
            
            if not csv_bruts:
                logger.info("‚è≠Ô∏è Aucun gros fichier CSV √† nettoyer")
                return True
            
            logger.info(f"üîç Trouv√© {len(csv_bruts)} fichiers CSV volumineux")
            
            # Utiliser le script de nettoyage
            script_nettoyage = self.paths['scripts_bdd'] / "livres" / "nettoyage_ultra.py"
            
            if script_nettoyage.exists():
                logger.info("üßπ Lancement nettoyage des donn√©es...")
                
                # Le script de nettoyage est souvent interactif
                logger.info("üí° Script de nettoyage disponible pour lancement manuel")
                self.etat['resultats']['nettoyage'] = {
                    'fichiers_detectes': len(csv_bruts),
                    'status': 'pr√™t'
                }
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Exception √©tape nettoyage: {e}")
            return False

    def executer_etape_postgresql(self) -> bool:
        """√âtape 4: Import PostgreSQL"""
        self.etat['etape_actuelle'] = 4
        logger.info("üóÑÔ∏è √âTAPE 4/8: Import PostgreSQL")
        
        try:
            script_postgres = self.paths['scripts_bdd'] / "livres" / "formatage_bdd_postgresql.py"
            
            if not script_postgres.exists():
                raise FileNotFoundError(f"Script PostgreSQL non trouv√©: {script_postgres}")
            
            # V√©rifier la disponibilit√© de PostgreSQL
            try:
                import psycopg2
                logger.info("‚úÖ Driver PostgreSQL disponible")
            except ImportError:
                logger.warning("‚ö†Ô∏è Driver PostgreSQL manquant (pip install psycopg2-binary)")
                return False
            
            # Chercher des fichiers CSV nettoy√©s
            csv_propres = []
            for pattern in ["*nettoye*.csv", "*propre*.csv", "*clean*.csv"]:
                csv_propres.extend(self.workspace.rglob(pattern))
            
            if csv_propres:
                csv_choisi = max(csv_propres, key=lambda x: x.stat().st_size)
                logger.info(f"üìä Fichier CSV s√©lectionn√©: {csv_choisi.name}")
                
                logger.info("üí° Import PostgreSQL pr√™t - configuration du sch√©ma n√©cessaire")
                self.etat['resultats']['postgresql'] = {
                    'fichier_source': str(csv_choisi),
                    'schema': self.config['bdd']['schema_postgres'],
                    'status': 'pr√™t'
                }
            else:
                logger.info("‚è≠Ô∏è Aucun fichier CSV nettoy√© trouv√© pour PostgreSQL")
                self.etat['resultats']['postgresql'] = {'status': 'aucun_fichier'}
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Exception √©tape PostgreSQL: {e}")
            return False

    def executer_etape_mongodb(self) -> bool:
        """√âtape 5: Import MongoDB"""
        self.etat['etape_actuelle'] = 5
        logger.info("üçÉ √âTAPE 5/8: Import MongoDB")
        
        try:
            script_mongo = self.paths['scripts_bdd'] / "nosql" / "import_mongodb.py"
            
            if not script_mongo.exists():
                raise FileNotFoundError(f"Script MongoDB non trouv√©: {script_mongo}")
            
            # V√©rifier la disponibilit√© de MongoDB
            try:
                import pymongo
                logger.info("‚úÖ Driver MongoDB disponible")
            except ImportError:
                logger.warning("‚ö†Ô∏è Driver MongoDB manquant (pip install pymongo)")
                return False
            
            # Chercher des fichiers JSON
            json_files = []
            json_dir = self.paths['data_json'] / "livres_json_ameliore"
            if json_dir.exists():
                json_files = list(json_dir.glob("*.json"))
            
            if json_files:
                total_size = sum(f.stat().st_size for f in json_files) / (1024*1024)
                logger.info(f"üìÑ Trouv√© {len(json_files)} fichiers JSON ({total_size:.1f} MB)")
                
                logger.info("üí° Import MongoDB pr√™t - v√©rification connexion n√©cessaire")
                self.etat['resultats']['mongodb'] = {
                    'fichiers_json': len(json_files),
                    'taille_mb': round(total_size, 1),
                    'status': 'pr√™t'
                }
            else:
                logger.info("‚è≠Ô∏è Aucun fichier JSON trouv√© pour MongoDB")
                self.etat['resultats']['mongodb'] = {'status': 'aucun_fichier'}
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Exception √©tape MongoDB: {e}")
            return False

    def verifier_environnement(self) -> bool:
        """√âtape 6: V√©rification de l'environnement"""
        self.etat['etape_actuelle'] = 6
        logger.info("‚úÖ √âTAPE 6/8: V√©rification environnement")
        
        verifications = {
            'python': sys.version_info >= (3, 8),
            'requests': self._verifier_module('requests'),
            'pandas': self._verifier_module('pandas'),
            'beautifulsoup4': self._verifier_module('bs4'),
            'sqlalchemy': self._verifier_module('sqlalchemy'),
            'psycopg2': self._verifier_module('psycopg2'),
            'pymongo': self._verifier_module('pymongo'),
        }
        
        logger.info("üîç V√©rification des d√©pendances:")
        for module, status in verifications.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            logger.info(f"   {status_icon} {module}")
        
        # V√©rifier les chemins
        logger.info("üìÇ V√©rification des chemins:")
        for nom, chemin in self.paths.items():
            exists = chemin.exists()
            status_icon = "‚úÖ" if exists else "‚ùå"
            logger.info(f"   {status_icon} {nom}: {chemin}")
        
        self.etat['resultats']['environnement'] = verifications
        return all(verifications.values())

    def _verifier_module(self, nom_module: str) -> bool:
        """V√©rifie si un module Python est disponible"""
        try:
            __import__(nom_module)
            return True
        except ImportError:
            return False

    def generer_rapport(self):
        """√âtape 7: G√©n√©ration du rapport final"""
        self.etat['etape_actuelle'] = 7
        logger.info("üìã √âTAPE 7/8: G√©n√©ration du rapport")
        
        rapport = {
            'timestamp': self.timestamp,
            'duree_totale': (datetime.now() - self.etat['debut']).total_seconds(),
            'config': self.config,
            'resultats': self.etat['resultats'],
            'erreurs': self.etat['erreurs']
        }
        
        # Sauvegarder le rapport
        fichier_rapport = self.workspace / f"rapport_pipeline_{self.timestamp}.json"
        with open(fichier_rapport, 'w', encoding='utf-8') as f:
            json.dump(rapport, f, indent=2, default=str)
        
        logger.info(f"üìÑ Rapport sauvegard√©: {fichier_rapport.name}")
        
        # Afficher le r√©sum√©
        print("\n" + "="*60)
        print("üìä R√âSUM√â DU PIPELINE")
        print("="*60)
        print(f"üïê Dur√©e totale: {rapport['duree_totale']:.1f} secondes")
        print(f"üìÇ Workspace: {self.workspace}")
        print(f"üè∑Ô∏è Timestamp: {self.timestamp}")
        print()
        
        for etape, resultat in self.etat['resultats'].items():
            print(f"‚Ä¢ {etape.upper()}: {resultat.get('status', 'termin√©')}")
            
        if self.etat['erreurs']:
            print(f"\n‚ö†Ô∏è {len(self.etat['erreurs'])} erreurs rencontr√©es")

    def nettoyer_temporaires(self):
        """√âtape 8: Nettoyage des fichiers temporaires"""
        self.etat['etape_actuelle'] = 8
        logger.info("üßπ √âTAPE 8/8: Nettoyage fichiers temporaires")
        
        # Patterns de fichiers temporaires √† nettoyer
        patterns_temp = [
            "*.tmp", "*.log", "*temp*", "__pycache__",
            "*.pyc", ".pytest_cache"
        ]
        
        fichiers_supprimes = 0
        for pattern in patterns_temp:
            for fichier in self.workspace.rglob(pattern):
                try:
                    if fichier.is_file():
                        fichier.unlink()
                        fichiers_supprimes += 1
                    elif fichier.is_dir():
                        shutil.rmtree(fichier)
                        fichiers_supprimes += 1
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Impossible de supprimer {fichier}: {e}")
        
        logger.info(f"üßπ {fichiers_supprimes} fichiers temporaires supprim√©s")

    def executer_pipeline_complet(self):
        """Ex√©cute le pipeline complet"""
        logger.info("üöÄ D√âMARRAGE PIPELINE COMPLET")
        print(f"\n{'='*60}")
        print("üéØ PIPELINE COMPLET DATABOOK")
        print(f"{'='*60}")
        
        etapes = [
            ("üì° R√©cup√©ration API", self.executer_etape_api),
            ("üï∑Ô∏è Scrapping Babelio", self.executer_etape_scrapping),
            ("üìä Nettoyage CSV", self.executer_etape_nettoyage_csv),
            ("üóÑÔ∏è PostgreSQL", self.executer_etape_postgresql),
            ("üçÉ MongoDB", self.executer_etape_mongodb),
            ("‚úÖ V√©rification", self.verifier_environnement),
            ("üìã Rapport", self.generer_rapport),
            ("üßπ Nettoyage", self.nettoyer_temporaires)
        ]
        
        for nom_etape, fonction_etape in etapes:
            print(f"\n{'-'*60}")
            print(f"‚ñ∂Ô∏è {nom_etape}")
            print(f"{'-'*60}")
            
            try:
                succes = fonction_etape()
                if succes:
                    logger.info(f"‚úÖ {nom_etape} termin√©e avec succ√®s")
                else:
                    logger.warning(f"‚ö†Ô∏è {nom_etape} termin√©e avec des avertissements")
            except Exception as e:
                logger.error(f"‚ùå Erreur dans {nom_etape}: {e}")
                self.etat['erreurs'].append(f"{nom_etape}: {e}")
        
        logger.info("üèÅ PIPELINE COMPLET TERMIN√â")

    def afficher_status(self):
        """Affiche le status actuel et les diagnostics"""
        print("\nüìã STATUS ET DIAGNOSTICS")
        print("="*50)
        
        # Informations g√©n√©rales
        print(f"üìÇ Workspace: {self.workspace}")
        print(f"üêç Python: {sys.version.split()[0]}")
        print(f"üïê Timestamp: {self.timestamp}")
        print()
        
        # V√©rifier les dossiers
        print("üìÅ STRUCTURE DES DOSSIERS:")
        for nom, chemin in self.paths.items():
            exists = chemin.exists()
            status = "‚úÖ" if exists else "‚ùå"
            size_info = ""
            
            if exists and chemin.is_dir():
                files_count = len(list(chemin.rglob("*")))
                size_info = f" ({files_count} fichiers)"
            
            print(f"   {status} {nom}: {chemin.name}{size_info}")
        
        print()
        
        # V√©rifier les donn√©es
        print("üìä DONN√âES DISPONIBLES:")
        
        # JSON
        json_dir = self.paths['data_json'] / "livres_json_ameliore"
        if json_dir.exists():
            json_files = list(json_dir.glob("*.json"))
            total_size = sum(f.stat().st_size for f in json_files) / (1024*1024)
            print(f"   üìÑ JSON: {len(json_files)} fichiers ({total_size:.1f} MB)")
        else:
            print("   üìÑ JSON: Aucun fichier")
        
        # CSV
        csv_files = list(self.workspace.rglob("*.csv"))
        csv_files = [f for f in csv_files if f.stat().st_size > 1024*1024]
        if csv_files:
            total_size = sum(f.stat().st_size for f in csv_files) / (1024*1024)
            print(f"   üìä CSV: {len(csv_files)} fichiers ({total_size:.1f} MB)")
        else:
            print("   üìä CSV: Aucun fichier volumineux")

def main():
    """Fonction principale"""
    try:
        pipeline = PipelineMaster()
        
        while True:
            choix = pipeline.afficher_menu_principal()
            
            if choix == "1":
                pipeline.executer_pipeline_complet()
            elif choix == "2":
                pipeline.executer_etape_api()
            elif choix == "3":
                pipeline.executer_etape_scrapping()
            elif choix == "4":
                pipeline.executer_etape_nettoyage_csv()
            elif choix == "5":
                pipeline.executer_etape_postgresql()
                pipeline.executer_etape_mongodb()
            elif choix == "6":
                pipeline.configurer_pipeline()
                continue
            elif choix == "7":
                pipeline.afficher_status()
                continue
            elif choix == "8":
                pipeline.nettoyer_temporaires()
                continue
            elif choix == "9":
                logger.info("üëã Au revoir !")
                break
            else:
                print("‚ùå Choix invalide")
                continue
            
            # Demander si on continue
            if input("\nContinuer? (o/N): ").lower() != 'o':
                break
    
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Pipeline interrompu par l'utilisateur")
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()