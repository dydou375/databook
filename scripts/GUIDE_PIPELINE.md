# ğŸš€ GUIDE PIPELINE DATABOOK

## ğŸ“‹ Vue d'ensemble

Le **Pipeline DataBook** est un systÃ¨me automatisÃ© qui orchestre toute la chaÃ®ne de rÃ©cupÃ©ration et traitement des donnÃ©es pour votre projet d'analyse de livres. Il combine :

- ğŸ“¡ **RÃ©cupÃ©ration API** (Google Books, OpenLibrary)
- ğŸ•·ï¸ **Scrapping web** (Babelio pour critiques)
- ğŸ“Š **Traitement CSV** (nettoyage, formatage)
- ğŸ—„ï¸ **Import PostgreSQL** (tables relationnelles)
- ğŸƒ **Import MongoDB** (documents JSON)

## ğŸ¯ DÃ©marrage rapide

### Option 1 : DÃ©marrage automatique
```bash
python demarrage_rapide.py --auto
```

### Option 2 : DÃ©marrage interactif
```bash
python demarrage_rapide.py
```

### Option 3 : Pipeline complet manuel
```bash
python pipeline_master.py
```

## âš™ï¸ Configurations disponibles

### ğŸš€ Configuration minimale
- **Usage** : Tests rapides, dÃ©veloppement
- **DonnÃ©es** : ~100 livres API, vÃ©rifications seulement
- **DurÃ©e** : 5-10 minutes
- **Espace** : ~50 MB

### ğŸ“Š Configuration standard
- **Usage** : Utilisation normale, dÃ©monstrations
- **DonnÃ©es** : ~1000 livres API, imports BDD
- **DurÃ©e** : 30-60 minutes
- **Espace** : ~500 MB

### ğŸ¯ Configuration complÃ¨te
- **Usage** : Production, analyses poussÃ©es
- **DonnÃ©es** : ~5000+ livres, scrapping Babelio
- **DurÃ©e** : 2-4 heures
- **Espace** : ~2 GB

## ğŸ“¦ PrÃ©requis

### Modules Python requis
```bash
pip install requests pandas beautifulsoup4 sqlalchemy psycopg2-binary pymongo
```

### Bases de donnÃ©es
- **PostgreSQL** : Pour donnÃ©es relationnelles
- **MongoDB** : Pour documents JSON

### Structure de dossiers attendue
```
databook/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ api/              # Scripts rÃ©cupÃ©ration API
â”‚   â””â”€â”€ scrapping/        # Scripts scrapping web
â”œâ”€â”€ bdd/
â”‚   â”œâ”€â”€ livres/          # Scripts PostgreSQL
â”‚   â””â”€â”€ nosql/           # Scripts MongoDB
â””â”€â”€ data/
    â”œâ”€â”€ livre_json/      # Fichiers JSON gÃ©nÃ©rÃ©s
    â””â”€â”€ fichier_openlibrary/  # DonnÃ©es OpenLibrary
```

## ğŸ”§ Utilisation dÃ©taillÃ©e

### 1. Pipeline complet automatique

```bash
# Lancement avec dÃ©tection automatique
python demarrage_rapide.py --auto

# Forcer une configuration spÃ©cifique
python demarrage_rapide.py --auto --config standard
```

### 2. Ã‰tapes individuelles

```python
from pipeline_master import PipelineMaster

pipeline = PipelineMaster()

# RÃ©cupÃ©ration API uniquement
pipeline.executer_etape_api()

# Scrapping Babelio
pipeline.executer_etape_scrapping()

# Import PostgreSQL + MongoDB
pipeline.executer_etape_postgresql()
pipeline.executer_etape_mongodb()
```

### 3. Menu interactif complet

```bash
python pipeline_master.py
```

**Options disponibles :**
1. ğŸš€ Pipeline complet (toutes Ã©tapes)
2. ğŸ“¡ RÃ©cupÃ©ration API uniquement
3. ğŸ•·ï¸ Scrapping Babelio uniquement
4. ğŸ“Š Traitement CSV uniquement
5. ğŸ—„ï¸ Import BDD uniquement
6. âš™ï¸ Configuration personnalisÃ©e
7. ğŸ“‹ Status et diagnostics
8. ğŸ§¹ Nettoyage donnÃ©es temporaires

## ğŸ“Š Ã‰tapes du pipeline

### Ã‰tape 1 : ğŸ“¡ RÃ©cupÃ©ration API
- **Sources** : Google Books API, OpenLibrary API
- **DonnÃ©es** : MÃ©tadonnÃ©es livres (titre, auteur, ISBN, etc.)
- **Format** : Fichiers JSON par catÃ©gorie
- **DurÃ©e** : 10-30 minutes selon configuration

### Ã‰tape 2 : ğŸ•·ï¸ Scrapping Babelio
- **Source** : Site web Babelio.com
- **DonnÃ©es** : Critiques, notes, rÃ©partition Ã©toiles
- **Format** : Fichiers JSON
- **DurÃ©e** : 20-120 minutes selon nombre de livres

### Ã‰tape 3 : ğŸ“Š Nettoyage CSV
- **EntrÃ©e** : Fichiers CSV bruts (OpenLibrary, exports)
- **Traitement** : DÃ©duplication, validation, formatage
- **Sortie** : CSV nettoyÃ©s, prÃªts pour BDD
- **DurÃ©e** : 5-15 minutes

### Ã‰tape 4 : ğŸ—„ï¸ Import PostgreSQL
- **Tables crÃ©Ã©es** : livre, auteur, editeur, langue, sujet + tables de liaison
- **SchÃ©ma** : Relationnel normalisÃ©
- **Performance** : ~1000 livres/minute
- **DurÃ©e** : 10-60 minutes selon volume

### Ã‰tape 5 : ğŸƒ Import MongoDB
- **Collection** : livres (documents JSON)
- **Structure** : Documents dÃ©normalisÃ©s
- **Index** : titre, auteurs
- **DurÃ©e** : 5-20 minutes

## ğŸ› ï¸ Configuration personnalisÃ©e

### Fichier de configuration
CrÃ©ez `config_pipeline.json` :

```json
{
  "api": {
    "max_livres_par_categorie": 1000,
    "categories_actives": 20,
    "delai_requetes": 2
  },
  "scrapping": {
    "max_livres_babelio": 500,
    "delai_scrapping": 3
  },
  "bdd": {
    "postgresql_url": "postgresql://user:pass@localhost:5432/databook",
    "mongodb_url": "mongodb://localhost:27017/",
    "schema_postgres": "mon_schema",
    "batch_size": 1000
  }
}
```

### Variables d'environnement
```bash
export DATABOOK_POSTGRES_URL="postgresql://..."
export DATABOOK_MONGO_URL="mongodb://..."
export DATABOOK_API_MAX_BOOKS=1000
```

## ğŸ“‹ Monitoring et logs

### Logs automatiques
- **Fichier** : `pipeline_master_YYYYMMDD_HHMMSS.log`
- **Format** : Timestamp, niveau, message
- **Niveaux** : INFO, WARNING, ERROR

### Rapport final
- **Fichier** : `rapport_pipeline_YYYYMMDD_HHMMSS.json`
- **Contenu** : Configuration, rÃ©sultats, erreurs, durÃ©es

### VÃ©rification status
```bash
python demarrage_rapide.py --status
```

## ğŸš¨ RÃ©solution des problÃ¨mes

### Erreurs courantes

#### Modules manquants
```bash
pip install requests pandas beautifulsoup4 sqlalchemy psycopg2-binary pymongo
```

#### PostgreSQL inaccessible
- VÃ©rifier que PostgreSQL est dÃ©marrÃ©
- ContrÃ´ler l'URL de connexion
- Tester : `psql -h localhost -U postgres`

#### MongoDB inaccessible
- VÃ©rifier que MongoDB est dÃ©marrÃ©
- ContrÃ´ler l'URL de connexion  
- Tester : `mongo --host localhost:27017`

#### Espace disque insuffisant
- Minimum recommandÃ© : 5 GB libre
- Utiliser configuration "minimale"
- Nettoyer fichiers temporaires

#### Timeouts API
- RÃ©duire `max_livres_par_categorie`
- Augmenter `delai_requetes`
- Relancer Ã©tape API individuellement

### RÃ©cupÃ©ration aprÃ¨s erreur

```python
# Reprendre Ã  partir d'une Ã©tape spÃ©cifique
pipeline = PipelineMaster()

# Par exemple, reprendre au nettoyage CSV
pipeline.executer_etape_nettoyage_csv()
pipeline.executer_etape_postgresql()
pipeline.executer_etape_mongodb()
```

### Nettoyage et reset

```bash
# Nettoyer fichiers temporaires
python pipeline_master.py  # Option 8

# Reset complet (supprimer toutes les donnÃ©es)
rm -rf data/livre_json/*
rm -f *.log *.json
```

## ğŸ“ˆ Optimisations

### Performance
- **CPU** : Scripts parallÃ©lisÃ©s automatiquement
- **MÃ©moire** : Traitement par lots (batch_size configurable)
- **RÃ©seau** : DÃ©lais configurables pour Ã©viter rate limiting

### Espace disque
- Configuration minimale : ~100 MB
- Configuration standard : ~1 GB  
- Configuration complÃ¨te : ~5 GB

### Personnalisation avancÃ©e

#### Modifier les catÃ©gories API
Ã‰ditez `scripts/api/recupÃ©ration_api_livre_amelioree.py` :
```python
categories_etendues = [
    {"fr": "Ma catÃ©gorie", "en": "My category", "variations": ["keyword1", "keyword2"]}
]
```

#### Ajouter sources de donnÃ©es
1. CrÃ©er script dans `scripts/api/`
2. Modifier `pipeline_master.py` pour inclure le script
3. Ajouter configuration correspondante

## ğŸ“ Exemples d'usage

### Recherche acadÃ©mique
```bash
python demarrage_rapide.py --config standard
# DonnÃ©es Ã©quilibrÃ©es pour analyses statistiques
```

### DÃ©veloppement d'application
```bash
python demarrage_rapide.py --config minimal
# Tests rapides pendant dÃ©veloppement
```

### Production / Analytics
```bash
python demarrage_rapide.py --config complet
# Maximum de donnÃ©es pour ML/BI
```

### Usage ponctuel API
```python
from pipeline_master import PipelineMaster
pipeline = PipelineMaster()
pipeline.config['api']['max_livres_par_categorie'] = 50
pipeline.executer_etape_api()
```

## ğŸ“ Support

### Diagnostics automatiques
```bash
python demarrage_rapide.py --status
```

### Logs dÃ©taillÃ©s
```bash
tail -f pipeline_master_*.log
```

### VÃ©rification environnement
```python
from pipeline_master import PipelineMaster
pipeline = PipelineMaster()
pipeline.verifier_environnement()
```

---

## ğŸ“ Notes importantes

- âš ï¸ **Respect des APIs** : Les dÃ©lais entre requÃªtes sont configurÃ©s pour respecter les limites des services
- ğŸ”’ **DonnÃ©es personnelles** : Aucune donnÃ©e personnelle n'est collectÃ©e ni stockÃ©e
- ğŸ“Š **Usage acadÃ©mique** : Pipeline conÃ§u pour la recherche et l'Ã©ducation
- ğŸ”„ **Mises Ã  jour** : Scripts mis Ã  jour rÃ©guliÃ¨rement pour s'adapter aux Ã©volutions des APIs

---

*Guide Pipeline DataBook v1.0 - 2025-01-27* 