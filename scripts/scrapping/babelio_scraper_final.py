import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

def extraire_repartition_notes(soup):
    """
    Extrait la r√©partition des notes (histogramme des √©toiles)
    """
    repartition = {}
    
    try:
        # M√©thode 1 : Chercher le texte complet de la page et analyser les patterns
        page_text = soup.get_text()
        
        # Patterns pour trouver la r√©partition (ex: "5‚òÖ 2 avis", "4‚òÖ 7 avis")
        star_patterns = [
            r'5‚òÖ\s*(\d+)\s*avis',
            r'4‚òÖ\s*(\d+)\s*avis', 
            r'3‚òÖ\s*(\d+)\s*avis',
            r'2‚òÖ\s*(\d+)\s*avis',
            r'1‚òÖ\s*(\d+)\s*avis'
        ]
        
        for i, pattern in enumerate(star_patterns):
            nb_etoiles = 5 - i  # 5, 4, 3, 2, 1
            match = re.search(pattern, page_text)
            if match:
                repartition[f"{nb_etoiles}_etoiles"] = int(match.group(1))
        
        # M√©thode 2 : Chercher dans les √©l√©ments sp√©cifiques
        # Analyser les lignes qui contiennent "avis" et des √©toiles
        all_text_elements = soup.find_all(text=re.compile(r'\d+\s*avis'))
        
        for text_elem in all_text_elements:
            parent = text_elem.parent
            if parent:
                # Chercher le contexte autour de ce texte
                context = parent.get_text()
                
                # Pour chaque nombre d'√©toiles, voir si on trouve un pattern
                for nb_etoiles in range(1, 6):
                    patterns = [
                        rf'{nb_etoiles}‚òÖ.*?(\d+)\s*avis',
                        rf'{nb_etoiles}\*.*?(\d+)\s*avis',
                        rf'{nb_etoiles}\s*√©toiles?.*?(\d+)\s*avis'
                    ]
                    
                    for pattern in patterns:
                        match = re.search(pattern, context)
                        if match and f"{nb_etoiles}_etoiles" not in repartition:
                            repartition[f"{nb_etoiles}_etoiles"] = int(match.group(1))
        
        # M√©thode 3 : Analyse sp√©cifique de la structure Babelio
        # Chercher dans les divs/spans qui pourraient contenir l'histogramme
        possible_containers = soup.find_all(['div', 'section', 'ul'], 
                                          class_=lambda x: x and any(keyword in x.lower() 
                                          for keyword in ['rating', 'note', 'avis', 'review', 'histogram']))
        
        for container in possible_containers:
            container_text = container.get_text()
            if 'avis' in container_text and any(f'{i}‚òÖ' in container_text for i in range(1, 6)):
                # Extraire tous les patterns d'avis dans ce conteneur
                avis_matches = re.findall(r'(\d+)‚òÖ.*?(\d+)\s*avis', container_text)
                for nb_etoiles, nb_avis in avis_matches:
                    if f"{nb_etoiles}_etoiles" not in repartition:
                        repartition[f"{nb_etoiles}_etoiles"] = int(nb_avis)
        
        # M√©thode 4 : Recherche dans les √©l√©ments avec attributs data
        data_elements = soup.find_all(attrs={"data-rating": True})
        for elem in data_elements:
            rating = elem.get('data-rating')
            # Chercher le nombre d'avis associ√©
            elem_text = elem.get_text()
            avis_match = re.search(r'(\d+)\s*avis', elem_text)
            if avis_match and rating:
                try:
                    rating_num = int(float(rating))
                    if 1 <= rating_num <= 5:
                        repartition[f"{rating_num}_etoiles"] = int(avis_match.group(1))
                except ValueError:
                    pass
        
        # S'assurer que toutes les √©toiles sont pr√©sentes (avec 0 si pas trouv√©)
        if repartition:
            for i in range(1, 6):
                if f"{i}_etoiles" not in repartition:
                    repartition[f"{i}_etoiles"] = 0
            
            print(f"üìä R√©partition trouv√©e : 5‚òÖ:{repartition.get('5_etoiles',0)}, 4‚òÖ:{repartition.get('4_etoiles',0)}, 3‚òÖ:{repartition.get('3_etoiles',0)}, 2‚òÖ:{repartition.get('2_etoiles',0)}, 1‚òÖ:{repartition.get('1_etoiles',0)}")
        
        return repartition if repartition else None
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'extraction de la r√©partition : {e}")
        return None

def extraire_note_critique(container):
    """
    Extrait la note d'une critique individuelle de mani√®re plus robuste
    """
    note_utilisateur = None
    
    try:
        # M√©thode 1 : Compter les images d'√©toiles
        stars_img = container.find_all("img", src=lambda x: x and ("etoile" in x or "star" in x))
        if stars_img:
            note_utilisateur = len(stars_img)
        
        # M√©thode 2 : Chercher dans les divs avec rateit
        if not note_utilisateur:
            rateit_div = container.find("div", class_=lambda x: x and "rateit" in x)
            if rateit_div:
                # Analyser les attributs data-rateit-value ou similaires
                data_value = rateit_div.get("data-rateit-value")
                if data_value:
                    try:
                        note_utilisateur = float(data_value)
                    except ValueError:
                        pass
                
                # Ou analyser la classe pour d√©terminer le nombre d'√©toiles
                if not note_utilisateur:
                    class_attr = rateit_div.get("class", [])
                    for cls in class_attr:
                        if "rateit-range" in cls:
                            # Extraire le nombre d'√©toiles de la classe
                            match = re.search(r'(\d+)', cls)
                            if match:
                                note_utilisateur = int(match.group(1))
        
        # M√©thode 3 : Chercher les spans avec classes d'√©toiles
        if not note_utilisateur:
            star_spans = container.find_all("span", class_=lambda x: x and ("star" in x or "etoile" in x))
            if star_spans:
                note_utilisateur = len(star_spans)
        
        # M√©thode 4 : Analyser le texte pour des patterns d'√©toiles
        if not note_utilisateur:
            text_content = container.get_text()
            star_patterns = [r'‚òÖ{1,5}', r'\*{1,5}', r'(\d+)/5', r'(\d+)\s*√©toiles?']
            for pattern in star_patterns:
                match = re.search(pattern, text_content)
                if match:
                    if pattern == r'(\d+)/5' or pattern == r'(\d+)\s*√©toiles?':
                        note_utilisateur = int(match.group(1))
                    else:
                        note_utilisateur = len(match.group())
                    break
        
        return note_utilisateur
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'extraction de la note de critique : {e}")
        return None

def extraire_isbn_13_by_csv(fichier_csv):
    """
    Extrait les ISBN 13 d'un fichier CSV
    """
    df = pd.read_csv(fichier_csv, 
                    encoding='latin-1',
                    sep=';',
                    on_bad_lines='skip',
                    engine='python'
                    )
    
    list_isbn = df['ISBN'].tolist()
    return list_isbn

def chercher_babelio_par_isbn(isbn13):
    """
    Recherche un livre sur Babelio √† partir de son ISBN
    """
    url = "https://www.babelio.com/recherche.php"
    
    # Donn√©es du formulaire POST (comme observ√© dans les DevTools)
    data = {
        'Recherche': isbn13,
        'recherche': ''
    }
    
    print(f"üîé Recherche de l'ISBN {isbn13} sur Babelio...")
    
    try:
        res = requests.post(url, headers=headers, data=data, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")

        # Chercher le lien vers la fiche du livre
        lien = soup.find("a", class_="titre1")
        if not lien:
            # Essayer d'autres s√©lecteurs
            liens_livres = soup.find_all("a", href=lambda x: x and "/livres/" in x)
            if liens_livres:
                lien = liens_livres[0]
                print("‚úÖ Lien trouv√© avec s√©lecteur alternatif")
            else:
                print("‚ùå Aucun lien vers une fiche livre trouv√©")
                return None
        else:
            print("‚úÖ Lien trouv√© avec class='titre1'")

        lien_fiche = "https://www.babelio.com" + lien["href"]
        return scraper_fiche_babelio(lien_fiche, isbn13)
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la recherche : {e}")
        return None

def scraper_fiche_babelio(url, isbn=None):
    """
    Extrait les informations d'une fiche livre Babelio
    """
    print(f"üìñ Extraction des donn√©es de : {url}")
    
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")

        # Titre du livre - cibler pr√©cis√©ment la structure HTML
        titre = None
        
        # M√©thode 1 : h1 avec itemprop="name" (structure principale)
        titre_tag = soup.find("h1", itemprop="name")
        if titre_tag:
            # Chercher le lien √† l'int√©rieur du h1
            lien_titre = titre_tag.find("a")
            if lien_titre:
                titre = lien_titre.get_text(strip=True)
            else:
                titre = titre_tag.get_text(strip=True)
        
        # M√©thode 2 : lien direct avec href contenant le titre
        if not titre:
            liens_livre = soup.find_all("a", href=lambda x: x and "/livres/" in x)
            for lien in liens_livre:
                texte_lien = lien.get_text(strip=True)
                # Prendre le premier lien non vide qui semble √™tre un titre (pas trop long)
                if texte_lien and len(texte_lien) < 100 and not any(mot in texte_lien.lower() for mot in ['critique', 'citation', 'forum', 'auteur']):
                    titre = texte_lien
                    break
        
        # M√©thode 3 : h1 dans la zone livre_header_con
        if not titre:
            div_header = soup.find("div", class_="livre_header_con")
            if div_header:
                titre_tag = div_header.find("h1")
                if titre_tag:
                    lien_titre = titre_tag.find("a")
                    if lien_titre:
                        titre = lien_titre.get_text(strip=True)
                    else:
                        titre = titre_tag.get_text(strip=True)
                
        # M√©thode 4 : h1 g√©n√©ral comme fallback
        if not titre:
            titre_tag = soup.find("h1")
            if titre_tag:
                titre = titre_tag.get_text(strip=True)
        
        print(f"üìö Titre : {'‚úÖ Trouv√©' if titre else '‚ùå Non trouv√©'} - {titre if titre else 'N/A'}")

        # Auteur
        auteur_tag = soup.find("a", href=lambda x: x and "/auteur/" in x)
        auteur = auteur_tag.get_text(strip=True) if auteur_tag else None
        print(f"‚úçÔ∏è Auteur : {'‚úÖ Trouv√©' if auteur else '‚ùå Non trouv√©'}")

        # R√©sum√© avec le bon s√©lecteur
        resume_tag = soup.find("div", class_="livre_resume")
        resume = resume_tag.get_text(strip=True) if resume_tag else None
        print(f"üìù R√©sum√© : {'‚úÖ Trouv√©' if resume else '‚ùå Non trouv√©'}")

        # Note
        note_tag = soup.find("span", itemprop="ratingValue")
        note = float(note_tag.text.strip().replace(",", ".")) if note_tag else None
        print(f"‚≠ê Note : {note if note else '‚ùå Non trouv√©e'}")

        # Nombre de votes
        votes_tag = soup.find("span", itemprop="ratingCount")
        if votes_tag:
            votes_txt = votes_tag.text.strip().split()[0].replace(" ", "")
            nombre_votes = int(votes_txt)
        else:
            nombre_votes = None
        print(f"üó≥Ô∏è Nombre de votes : {nombre_votes if nombre_votes else '‚ùå Non trouv√©'}")

        # R√©partition des notes (histogramme)
        repartition_notes = extraire_repartition_notes(soup)
        print(f"üìä R√©partition des notes : {'‚úÖ Trouv√©e' if repartition_notes else '‚ùå Non trouv√©e'}")

        # Extraction des critiques
        critiques = []
        
        # Chercher les conteneurs de critiques
        critiques_containers = soup.find_all("div", class_="post_con")
        
        print(f"üí¨ Recherche des critiques... {len(critiques_containers)} trouv√©es")
        
        for container in critiques_containers:
            try:
                critique_data = {}
                
                # Nom d'utilisateur - plusieurs approches am√©lior√©es
                utilisateur = None
                
                # M√©thode 1 : lien vers monprofil.php
                user_link = container.find("a", href=lambda x: x and "/monprofil.php" in x)
                if user_link:
                    utilisateur = user_link.text.strip()
                
                # M√©thode 2 : span avec itemprop="name" dans le contexte d'une critique
                if not utilisateur:
                    # Chercher sp√©cifiquement dans la zone de l'auteur de la critique
                    name_spans = container.find_all("span", itemprop="name")
                    for span in name_spans:
                        # V√©rifier que c'est bien dans le contexte d'un utilisateur (pas le titre du livre)
                        parent_context = span.parent
                        if parent_context:
                            # Si le span est dans un lien ou pr√®s d'un lien vers un profil
                            nearby_link = parent_context.find("a", href=lambda x: x and ("/monprofil.php" in x or "/membre/" in x))
                            if nearby_link or "user" in parent_context.get("class", []):
                                utilisateur = span.text.strip()
                                break
                            
                            # Ou si c'est dans une zone avec classe contenant "user", "auteur", "membre"
                            parent_classes = " ".join(parent_context.get("class", []))
                            if any(keyword in parent_classes.lower() for keyword in ["user", "auteur", "membre", "critique"]):
                                text_content = span.text.strip()
                                # √âviter les noms trop longs (probablement des titres)
                                if text_content and len(text_content) < 50:
                                    utilisateur = text_content
                                    break
                
                # M√©thode 3 : chercher dans les liens avec des classes sp√©cifiques
                if not utilisateur:
                    user_links = container.find_all("a", class_=lambda x: x and any(keyword in " ".join(x).lower() for keyword in ["user", "membre", "auteur", "lien", "croco"]))
                    for link in user_links:
                        text = link.text.strip()
                        if text and len(text) < 50:  # Nom d'utilisateur raisonnable
                            utilisateur = text
                            break
                
                # M√©thode 4 : analyser les √©l√©ments avec attribut itemprop="author" 
                # (structure visible dans votre capture : <span itemprop="author"><span itemprop="name">CatF</span></span>)
                if not utilisateur:
                    author_spans = container.find_all("span", itemprop="author")
                    for span in author_spans:
                        # Chercher d'abord un span avec itemprop="name" √† l'int√©rieur
                        name_span = span.find("span", itemprop="name")
                        if name_span:
                            utilisateur = name_span.text.strip()
                            break
                        
                        # Sinon chercher un lien
                        author_link = span.find("a")
                        if author_link:
                            utilisateur = author_link.text.strip()
                            break
                        else:
                            # Parfois le nom est directement dans le span
                            text = span.text.strip()
                            if text and len(text) < 50:
                                utilisateur = text
                                break
                
                if utilisateur:
                    critique_data["utilisateur"] = utilisateur
                
                # Date de la critique - plusieurs approches
                # Approche 1 : span avec style color:grey
                date_span = container.find("span", style=lambda x: x and "color:grey" in x)
                if date_span:
                    critique_data["date"] = date_span.text.strip()
                else:
                    # Approche 2 : chercher une date dans le texte (format fran√ßais)
                    text_content = container.get_text()
                    date_pattern = r'\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}'
                    date_match = re.search(date_pattern, text_content, re.IGNORECASE)
                    if date_match:
                        critique_data["date"] = date_match.group()
                    else:
                        # Approche 3 : chercher dans les divs avec classe "entete_date"
                        date_div = container.find("div", class_="entete_date")
                        if date_div:
                            critique_data["date"] = date_div.text.strip()
                
                # Note de l'utilisateur (nombre d'√©toiles) - utilisation de la fonction am√©lior√©e
                note_utilisateur = extraire_note_critique(container)
                if note_utilisateur:
                    critique_data["note_utilisateur"] = note_utilisateur
                
                # Texte de la critique
                # Chercher d'abord dans les divs sp√©cifiques
                texte_critique = ""
                
                # M√©thode 1 : chercher dans les divs sans classe ou avec certaines classes
                content_divs = container.find_all("div")
                for div in content_divs:
                    div_text = div.get_text(strip=True)
                    # Ignorer les divs qui contiennent des infos de m√©tadonn√©es
                    if (div_text and len(div_text) > 50 and 
                        not any(word in div_text.lower() for word in ['profil', 'critique', 'note', '√©toile', 'commentaire'])):
                        if len(div_text) > len(texte_critique):
                            texte_critique = div_text
                
                # M√©thode 2 : si pas trouv√©, prendre le texte principal du container
                if not texte_critique:
                    full_text = container.get_text(separator=' ', strip=True)
                    # Nettoyer le texte en supprimant les m√©tadonn√©es
                    lines = full_text.split('\n')
                    for line in lines:
                        line = line.strip()
                        if len(line) > 100:  # Prendre une ligne suffisamment longue
                            texte_critique = line
                            break
                
                if texte_critique:
                    critique_data["texte"] = texte_critique
                
                # Ajouter la critique si elle contient au moins un utilisateur
                if "utilisateur" in critique_data:
                    critiques.append(critique_data)
                    print(f"‚úÖ Critique trouv√©e : {critique_data.get('utilisateur', 'Anonyme')} - {critique_data.get('date', 'Sans date')}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction d'une critique : {e}")
                continue
        
        print(f"üí¨ {len(critiques)} critiques extraites avec succ√®s")

        # Structure des donn√©es compl√®te
        data_livre = {
            "isbn": isbn,
            "titre": titre,
            "auteur": auteur,
            "resume_babelio": resume,
            "note_babelio": note,
            "nombre_votes_babelio": nombre_votes,
            "repartition_notes_babelio": repartition_notes,
            "critiques_babelio": critiques,
            "url_babelio": url,
            "date_extraction": datetime.now().isoformat(),
            "nombre_critiques": len(critiques)
        }

        return data_livre
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de la fiche : {e}")
        return None

def sauvegarder_json(donnees, nom_fichier="babelio_data.json"):
    """
    Sauvegarde les donn√©es extraites dans un fichier JSON
    """
    try:
        # Cr√©er le dossier dans le m√™me r√©pertoire que le script
        script_dir = os.path.dirname(os.path.abspath(__file__))
        dossier = os.path.join(script_dir, "data_extraite")
        if not os.path.exists(dossier):
            os.makedirs(dossier)
        
        chemin_fichier = os.path.join(dossier, nom_fichier)
        
        with open(chemin_fichier, 'w', encoding='utf-8') as f:
            json.dump(donnees, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Donn√©es sauvegard√©es dans : {chemin_fichier}")
        return chemin_fichier
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {e}")
        return None

def charger_json_existant(nom_fichier="babelio_data.json"):
    """
    Charge un fichier JSON existant ou retourne une liste vide
    """
    try:
        # Chercher le fichier dans le m√™me r√©pertoire que le script
        script_dir = os.path.dirname(os.path.abspath(__file__))
        chemin_fichier = os.path.join(script_dir, "data_extraite", nom_fichier)
        if os.path.exists(chemin_fichier):
            with open(chemin_fichier, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du chargement du fichier existant : {e}")
        return []

def traiter_liste_isbn(liste_isbn, nom_fichier="babelio_data.json", delai=2):
    """
    Traite une liste d'ISBN et sauvegarde les r√©sultats en JSON
    """
    print(f"üöÄ Traitement de {len(liste_isbn)} ISBN...")
    
    # Charger les donn√©es existantes
    donnees_existantes = charger_json_existant(nom_fichier)
    isbn_deja_traites = {livre.get('isbn') for livre in donnees_existantes if isinstance(livre, dict)}
    
    resultats = donnees_existantes.copy()
    
    for i, isbn in enumerate(liste_isbn, 1):
        print(f"\n--- Traitement {i}/{len(liste_isbn)} : ISBN {isbn} ---")
        
        # V√©rifier si l'ISBN a d√©j√† √©t√© trait√©
        if isbn in isbn_deja_traites:
            print(f"‚è≠Ô∏è ISBN {isbn} d√©j√† trait√©, passage au suivant")
            continue
        
        data_livre = chercher_babelio_par_isbn(isbn)
        
        if data_livre:
            resultats.append(data_livre)
            print(f"‚úÖ Donn√©es ajout√©es pour {data_livre.get('titre', 'Titre inconnu')}")
            
            # Sauvegarde interm√©diaire toutes les 5 extractions
            if i % 5 == 0:
                sauvegarder_json(resultats, nom_fichier)
                print(f"üíæ Sauvegarde interm√©diaire effectu√©e ({i} livres trait√©s)")
        else:
            # Ajouter une entr√©e pour signaler l'√©chec
            resultats.append({
                "isbn": isbn,
                "erreur": "Donn√©es non trouv√©es sur Babelio",
                "date_extraction": datetime.now().isoformat()
            })
            print(f"‚ùå Aucune donn√©e trouv√©e pour l'ISBN {isbn}")
        
        # D√©lai entre les requ√™tes pour √©viter de surcharger le serveur
        if i < len(liste_isbn):
            print(f"‚è≥ Pause de {delai} secondes...")
            time.sleep(delai)
    
    # Sauvegarde finale
    chemin_final = sauvegarder_json(resultats, nom_fichier)
    
    # Statistiques finales
    livres_avec_donnees = sum(1 for livre in resultats if isinstance(livre, dict) and 'titre' in livre)
    livres_avec_erreur = sum(1 for livre in resultats if isinstance(livre, dict) and 'erreur' in livre)
    
    print(f"\nüìä R√âSUM√â FINAL :")
    print(f"üìö Livres avec donn√©es compl√®tes : {livres_avec_donnees}")
    print(f"‚ùå Livres avec erreurs : {livres_avec_erreur}")
    print(f"üìÅ Fichier de sauvegarde : {chemin_final}")
    
    return resultats

def afficher_statistiques_json(nom_fichier="babelio_data.json"):
    """
    Affiche des statistiques sur les donn√©es JSON
    """
    donnees = charger_json_existant(nom_fichier)
    
    if not donnees:
        print("‚ùå Aucune donn√©e trouv√©e")
        return
    
    print(f"\nüìä STATISTIQUES DU FICHIER {nom_fichier} :")
    print("=" * 60)
    
    total_livres = len(donnees)
    livres_avec_donnees = sum(1 for livre in donnees if isinstance(livre, dict) and 'titre' in livre)
    livres_avec_erreur = sum(1 for livre in donnees if isinstance(livre, dict) and 'erreur' in livre)
    
    print(f"üìö Total d'entr√©es : {total_livres}")
    print(f"‚úÖ Livres avec donn√©es : {livres_avec_donnees}")
    print(f"‚ùå Livres avec erreurs : {livres_avec_erreur}")
    
    if livres_avec_donnees > 0:
        # Statistiques sur les notes
        notes = [livre.get('note_babelio') for livre in donnees if isinstance(livre, dict) and livre.get('note_babelio')]
        if notes:
            note_moyenne = sum(notes) / len(notes)
            print(f"‚≠ê Note moyenne : {note_moyenne:.2f}")
            print(f"‚≠ê Note min/max : {min(notes):.1f} / {max(notes):.1f}")
        
        # Statistiques sur les critiques
        total_critiques = sum(livre.get('nombre_critiques', 0) for livre in donnees if isinstance(livre, dict))
        print(f"üí¨ Total de critiques extraites : {total_critiques}")
        
        if livres_avec_donnees > 0:
            moyenne_critiques = total_critiques / livres_avec_donnees
            print(f"üí¨ Moyenne de critiques par livre : {moyenne_critiques:.1f}")

def main():
    """
    Fonction principale avec menu interactif
    """
    print("=== üîç Scraper Babelio - Version JSON ===")
    print("1. Rechercher un ISBN unique")
    print("2. Traiter un fichier CSV d'ISBN")
    print("3. Afficher les statistiques du fichier JSON")
    print("4. Quitter")
    
    while True:
        choix = input("\nChoisissez une option (1-4) : ").strip()
        
        if choix == "1":
            # Recherche d'un ISBN unique
            isbn = input("Veuillez entrer un ISBN : ").strip()
            
            if not isbn:
                print("‚ùå Aucun ISBN saisi")
                continue
            
            data_livre = chercher_babelio_par_isbn(isbn)
            
            if data_livre:
                print(f"\n‚úÖ Informations trouv√©es pour l'ISBN {isbn}")
                
                # Sauvegarder en JSON
                nom_fichier = f"livre_{isbn}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                sauvegarder_json([data_livre], nom_fichier)
                
                # Affichage r√©sum√©
                print(f"üìö Titre : {data_livre.get('titre', 'N/A')}")
                print(f"‚úçÔ∏è Auteur : {data_livre.get('auteur', 'N/A')}")
                print(f"‚≠ê Note : {data_livre.get('note_babelio', 'N/A')}")
                print(f"üí¨ Critiques : {data_livre.get('nombre_critiques', 0)}")
                
            else:
                print(f"‚ö†Ô∏è Aucune information trouv√©e pour l'ISBN {isbn}")
        
        elif choix == "2":
            # Traitement d'un fichier CSV
            fichier_csv = input("Chemin vers le fichier CSV : ").strip()
            
            if not os.path.exists(fichier_csv):
                print("‚ùå Fichier introuvable")
                continue
            
            try:
                liste_isbn = extraire_isbn_13_by_csv(fichier_csv)
                print(f"üìã {len(liste_isbn)} ISBN trouv√©s dans le fichier")
                
                nom_fichier = input("Nom du fichier JSON de sortie (par d√©faut: babelio_data.json) : ").strip()
                if not nom_fichier:
                    nom_fichier = "babelio_data.json"
                
                delai = input("D√©lai entre les requ√™tes en secondes (par d√©faut: 2) : ").strip()
                try:
                    delai = int(delai) if delai else 2
                except ValueError:
                    delai = 2
                
                traiter_liste_isbn(liste_isbn, nom_fichier, delai)
                
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement du fichier CSV : {e}")
        
        elif choix == "3":
            # Affichage des statistiques
            nom_fichier = input("Nom du fichier JSON (par d√©faut: babelio_data.json) : ").strip()
            if not nom_fichier:
                nom_fichier = "babelio_data.json"
            
            afficher_statistiques_json(nom_fichier)
        
        elif choix == "4":
            print("üëã Au revoir !")
            break
        
        else:
            print("‚ùå Option invalide, veuillez choisir entre 1 et 4")

if __name__ == "__main__":
    main()